{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8006342,"sourceType":"datasetVersion","datasetId":4715399},{"sourceId":8053320,"sourceType":"datasetVersion","datasetId":4749520}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy requests nlpaug","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:31:20.486194Z","iopub.execute_input":"2024-05-06T02:31:20.486626Z","iopub.status.idle":"2024-05-06T02:31:32.640707Z","shell.execute_reply.started":"2024-05-06T02:31:20.486566Z","shell.execute_reply":"2024-05-06T02:31:32.639708Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\nRequirement already satisfied: nlpaug in /opt/conda/lib/python3.10/site-packages (1.1.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2024.2.2)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.1.4)\nRequirement already satisfied: gdown>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (5.1.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nltk>=3.4.5","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:32:54.694144Z","iopub.execute_input":"2024-05-06T02:32:54.694702Z","iopub.status.idle":"2024-05-06T02:33:06.423056Z","shell.execute_reply.started":"2024-05-06T02:32:54.694645Z","shell.execute_reply":"2024-05-06T02:33:06.421798Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:33:08.925113Z","iopub.execute_input":"2024-05-06T02:33:08.925521Z","iopub.status.idle":"2024-05-06T02:33:20.980519Z","shell.execute_reply.started":"2024-05-06T02:33:08.925492Z","shell.execute_reply":"2024-05-06T02:33:20.979352Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from nlpaug.util.file.download import DownloadUtil\nDownloadUtil.download_glove(model_name='glove.840B.300d', dest_dir='.') # Download GloVe model\n!pip install gensim>=4.1.2","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:34:27.444162Z","iopub.execute_input":"2024-05-06T02:34:27.444911Z","iopub.status.idle":"2024-05-06T02:42:17.247592Z","shell.execute_reply.started":"2024-05-06T02:34:27.444877Z","shell.execute_reply":"2024-05-06T02:42:17.246261Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install https://github.com/kpu/kenlm/archive/master.zip\n!pip install pyskiplist\n!pip install fitlog","metadata":{"execution":{"iopub.status.busy":"2024-05-06T02:58:31.918152Z","iopub.execute_input":"2024-05-06T02:58:31.919617Z","iopub.status.idle":"2024-05-06T03:00:11.583459Z","shell.execute_reply.started":"2024-05-06T02:58:31.919575Z","shell.execute_reply":"2024-05-06T03:00:11.582247Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting https://github.com/kpu/kenlm/archive/master.zip\n  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.6/553.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: kenlm\n  Building wheel for kenlm (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=592712 sha256=a986a7bc811a1b4a66a3c67809971219b94fd5cff7e25d571317270b306ce525\n  Stored in directory: /tmp/pip-ephem-wheel-cache-tiz0wi5w/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\nSuccessfully built kenlm\nInstalling collected packages: kenlm\nSuccessfully installed kenlm-0.2.0\nCollecting pyskiplist\n  Downloading pyskiplist-1.0.0-py2.py3-none-any.whl.metadata (640 bytes)\nDownloading pyskiplist-1.0.0-py2.py3-none-any.whl (8.6 kB)\nInstalling collected packages: pyskiplist\nSuccessfully installed pyskiplist-1.0.0\nCollecting fitlog\n  Downloading fitlog-0.9.15.tar.gz (937 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m937.6/937.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from fitlog) (0.6.2)\nRequirement already satisfied: flask>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from fitlog) (3.0.2)\nRequirement already satisfied: numpy>=1.16.4 in /opt/conda/lib/python3.10/site-packages (from fitlog) (1.26.4)\nRequirement already satisfied: gitpython>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from fitlog) (3.1.41)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flask>=1.0.2->fitlog) (3.0.1)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask>=1.0.2->fitlog) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask>=1.0.2->fitlog) (2.1.2)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask>=1.0.2->fitlog) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask>=1.0.2->fitlog) (1.7.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython>=3.1.2->fitlog) (4.0.11)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask>=1.0.2->fitlog) (2.1.3)\nBuilding wheels for collected packages: fitlog\n  Building wheel for fitlog (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fitlog: filename=fitlog-0.9.15-py3-none-any.whl size=969346 sha256=6797d1fdaf8dd9df7e5f2de0d90cc9766b65e4e61a9383b14648f886603447df\n  Stored in directory: /root/.cache/pip/wheels/75/ad/23/b9fde8d32b7ac748b74171c213b9f0b1af76aa430b3b973d55\nSuccessfully built fitlog\nInstalling collected packages: fitlog\nSuccessfully installed fitlog-0.9.15\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom pyskiplist import SkipList\nimport fitlog\nimport kenlm\nimport statistics","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:02:27.586100Z","iopub.execute_input":"2024-05-06T03:02:27.586929Z","iopub.status.idle":"2024-05-06T03:02:29.477803Z","shell.execute_reply.started":"2024-05-06T03:02:27.586896Z","shell.execute_reply":"2024-05-06T03:02:29.477050Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport argparse\n\nimport torch\nimport torch.nn as nn\nfrom torch import cuda\nimport torch.nn.functional as F\nfrom transformers import T5Tokenizer\n\nsys.path.append(\".\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:02:30.467911Z","iopub.execute_input":"2024-05-06T03:02:30.468482Z","iopub.status.idle":"2024-05-06T03:02:32.685179Z","shell.execute_reply.started":"2024-05-06T03:02:30.468451Z","shell.execute_reply":"2024-05-06T03:02:32.684367Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def collate_fn(insts, pad_token_id=1):\n    ''' Pad the instance to the max seq length in batch '''\n\n    max_len = max(len(inst) for inst in insts)\n    max_len = max_len if max_len > 4 else 5\n\n    batch_seq = np.array([\n        inst + [pad_token_id] * (max_len - len(inst))\n        for inst in insts])\n    batch_seq = torch.LongTensor(batch_seq)\n\n    return batch_seq","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:02:33.539874Z","iopub.execute_input":"2024-05-06T03:02:33.541012Z","iopub.status.idle":"2024-05-06T03:02:33.548450Z","shell.execute_reply.started":"2024-05-06T03:02:33.540977Z","shell.execute_reply":"2024-05-06T03:02:33.547560Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CNNDataset(torch.utils.data.Dataset):\n    def __init__(self, insts, label):\n        self.insts = insts\n        self.label = label\n\n    def __getitem__(self, index):\n        return self.insts[index], self.label[index]\n\n    def __len__(self):\n        return len(self.insts)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:02:34.771124Z","iopub.execute_input":"2024-05-06T03:02:34.771831Z","iopub.status.idle":"2024-05-06T03:02:34.777772Z","shell.execute_reply.started":"2024-05-06T03:02:34.771791Z","shell.execute_reply":"2024-05-06T03:02:34.776502Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#to evaluate the style classifier \ndef evaluate_sc(model, valid_loader, loss_fn, epoch):\n    '''Evaluation function for style classifier'''\n    model.eval()\n    total_acc = 0.\n    total_num = 0.\n    total_loss = 0.\n    with torch.no_grad():\n        for batch in valid_loader:\n            x_batch, y_batch = map(lambda x: x.to(device), batch)\n            logits = model(x_batch)\n            total_loss += loss_fn(logits, y_batch)\n            _, y_hat = torch.max(logits,dim=-1)\n            same = [float(p == q) for p, q in zip(y_batch, y_hat)]\n#             print(\"same for evaluation:\",same)\n            total_acc += sum(same)\n            total_num += len(y_batch)\n    model.train()\n    print('[Info] Epoch {:02d}-valid: {}'.format(\n                epoch, 'acc {:.4f}% | loss {:.4f}').format(\n        total_acc / total_num * 100, total_loss / total_num))\n\n    return total_acc / total_num, total_loss / total_num","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:02:36.162470Z","iopub.execute_input":"2024-05-06T03:02:36.162861Z","iopub.status.idle":"2024-05-06T03:02:36.172609Z","shell.execute_reply.started":"2024-05-06T03:02:36.162832Z","shell.execute_reply":"2024-05-06T03:02:36.171708Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# to iterate over the style classifier\ndef SCIterator(insts_0, insts_1, opt, pad_token_id=1, shuffle=True):\n    '''Data iterator for style classifier'''\n\n    def cls_fn(insts):\n        insts, labels = list(zip(*insts))\n        seq = collate_fn(insts, pad_token_id)\n        labels = torch.LongTensor(labels)\n        return (seq, labels)\n\n    num = len(insts_0) + len(insts_1)\n    loader = torch.utils.data.DataLoader(\n        CNNDataset(\n            insts=insts_0 + insts_1,\n            label=[0 if i < len(insts_0)\n                   else 1 for i in range(num)]),\n        shuffle=shuffle,\n        num_workers=2,\n        collate_fn=cls_fn,\n        batch_size=opt.batch_size)\n\n    return loader","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:04:59.588868Z","iopub.execute_input":"2024-05-06T03:04:59.589796Z","iopub.status.idle":"2024-05-06T03:04:59.596963Z","shell.execute_reply.started":"2024-05-06T03:04:59.589765Z","shell.execute_reply":"2024-05-06T03:04:59.595834Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\ndef load_embedding(tokenizer, embed_dim, embed_path=None):\n    '''Parse an embedding text file into an array.'''\n    sys.path.append(\".\")\n    embedding = np.random.normal(scale=embed_dim ** -0.5,\n                                 size=(len(tokenizer), embed_dim))\n    if embed_path is None:\n        return embedding\n\n    print('[Info] Loading embedding')\n    embed_dict = {}\n    with open(embed_path, 'r') as file:\n        for i, line in enumerate(file):\n            if i == 0:\n                continue\n            tokens = line.rstrip().split()\n            try:\n                embed_dict[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n            except:\n                continue\n\n    for i in range(len(tokenizer)):\n        try:\n            word = tokenizer.decode(i)\n            if word in embed_dict:\n                embedding[i] = embed_dict[word]\n        except:\n            print(i)\n\n    return embedding","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:00.809018Z","iopub.execute_input":"2024-05-06T03:05:00.809382Z","iopub.status.idle":"2024-05-06T03:05:00.817993Z","shell.execute_reply.started":"2024-05-06T03:05:00.809353Z","shell.execute_reply":"2024-05-06T03:05:00.817024Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\n\n\nclass ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, lr, decay_step = 1000, \n                       decay_rate=0.9, steps=0):\n        self.init_lr = lr\n        self.steps = steps\n        self._optimizer = optimizer\n        self.decay_rate = decay_rate\n        self.decay_step = decay_step\n\n    def step(self):\n        '''Step with the inner optimizer'''\n        self._update_learning_rate()\n        self._optimizer.step()\n\n    def zero_grad(self):\n        \"Zero out the gradients by the inner optimizer\"\n        self._optimizer.zero_grad()\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n        self.steps += 1\n        if self.steps >= self.decay_step:\n            lr = self.init_lr * math.pow(self.decay_rate, \n                                         int(self.steps / self.decay_step))\n            for param_group in self._optimizer.param_groups:\n                param_group['lr'] = lr\n        else:\n            for param_group in self._optimizer.param_groups:\n                param_group['lr'] = self.init_lr","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:01.940514Z","iopub.execute_input":"2024-05-06T03:05:01.941356Z","iopub.status.idle":"2024-05-06T03:05:01.949569Z","shell.execute_reply.started":"2024-05-06T03:05:01.941324Z","shell.execute_reply":"2024-05-06T03:05:01.948597Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sys.path.append(\".\")\n\nfilter_sizes = [1, 2, 3, 4, 5]\nnum_filters = [128, 128, 128, 128, 128]\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nspecial_tokens = [{'bos_token': '<bos>'},\n                  {'eos_token': '<eos>'}, {'sep_token': '<sep>'},\n                  {'pad_token': '<pad>'}, {'unk_token': '<unk>'}]\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, embeding):\n        super(EmbeddingLayer, self).__init__()\n        self.embeding = nn.Embedding(vocab_size, embed_dim)\n        if embeding is not None:\n            self.embeding.weight.data = torch.FloatTensor(embeding)\n\n    def forward(self, x):\n        if len(x.size()) == 2:\n            y = self.embeding(x)\n        else:\n            y = torch.matmul(x, self.embeding.weight)\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:03.033363Z","iopub.execute_input":"2024-05-06T03:05:03.034388Z","iopub.status.idle":"2024-05-06T03:05:03.064467Z","shell.execute_reply.started":"2024-05-06T03:05:03.034348Z","shell.execute_reply":"2024-05-06T03:05:03.063355Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class TextCNN(nn.Module):\n    '''A style classifier TextCNN'''\n\n    def __init__(self, embed_dim, vocab_size, filter_sizes, \n                 num_filters, embedding=None, dropout=0.0):\n        super(TextCNN, self).__init__()\n\n        self.feature_dim = sum(num_filters)\n        self.embeder = EmbeddingLayer(vocab_size, embed_dim, embedding)\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, n, (f, embed_dim))\n            for (n, f) in zip(num_filters, filter_sizes)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Sequential(\n            self.dropout,\n            nn.Linear(self.feature_dim, int(self.feature_dim / 2)), nn.ReLU(),\n            nn.Linear(int(self.feature_dim / 2), 2)\n        )\n    def forward(self, inp):\n        inp = self.embeder(inp).unsqueeze(1)\n        convs = [F.relu(conv(inp)).squeeze(3) for conv in self.convs]\n        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]\n        out = torch.cat(pools, 1)\n        logit = self.fc(out)\n\n        return logit\n\n    def build_embeder(self, vocab_size, embed_dim, embedding=None):\n        embeder = nn.Embedding(vocab_size, embed_dim)\n        nn.init.normal_(embeder.weight, mean=0, std=embed_dim ** -0.5)\n        if embedding is not None:\n            embeder.weight.data = torch.FloatTensor(embedding)\n\n        return embeder","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:03.953523Z","iopub.execute_input":"2024-05-06T03:05:03.954369Z","iopub.status.idle":"2024-05-06T03:05:03.965125Z","shell.execute_reply.started":"2024-05-06T03:05:03.954337Z","shell.execute_reply":"2024-05-06T03:05:03.964118Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser('Style Classifier TextCNN')\nparser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\nparser.add_argument('-dataset', default='em', type=str, help='the name of dataset')\nparser.add_argument('-embed_dim', default=300, type=int, help='the embedding size')\nparser.add_argument('-seed', default=42, type=int, help='pseudo random number seed')\nparser.add_argument('-min_count', default=0, type=int, help='minmum number of corpus')\nparser.add_argument(\"-dropout\", default=0.5, type=float, help=\"Keep prob in dropout.\")\nparser.add_argument('-max_len', default=50, type=int, help='maximum tokens in a batch')\nparser.add_argument('-log_step', default=100, type=int, help='print log every x steps')\nparser.add_argument('-eval_step', default=1000, type=int, help='early stopping training')\nparser.add_argument('-batch_size', default=32, type=int, help='maximum sents in a batch')\nparser.add_argument('-epoch', default=50, type=int, help='force stop at specified epoch')\n\nopt = parser.parse_args(['-lr', '0.001', '-dataset', 'em', '-embed_dim', '300', '-seed', '42', '-min_count', '0', '-dropout', '0.5', '-max_len', '50', '-log_step', '100', '-eval_step', '1000', '-batch_size', '32', '-epoch', '50'])\ntorch.manual_seed(opt.seed)\n\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\ntrain_src, train_tgt, valid_src, valid_tgt = [], [], [], []\ntrain_dict={}\nwith open('/kaggle/input/dataproject/data/{}/train/informal'.format(opt.dataset),'r') as f:\n    for line in f.readlines():\n        t=tokenizer.encode(line.strip())[:opt.max_len]\n        train_dict[line]=t\n        train_src.append(t)\nwith open('/kaggle/input/dataproject/data/{}/train/formal'.format(opt.dataset),'r') as f:\n    for line in f.readlines():\n        train_tgt.append(tokenizer.encode(line.strip())[:opt.max_len])\nwith open('/kaggle/input/dataproject/data/{}/tune/informal'.format(opt.dataset),'r') as f:\n    for line in f.readlines():\n        valid_src.append(tokenizer.encode(line.strip())[:opt.max_len])\nwith open('/kaggle/input/dataproject/data/{}/tune/formal'.format(opt.dataset),'r') as f:\n    for line in f.readlines():\n        valid_tgt.append(tokenizer.encode(line.strip())[:opt.max_len])\n\n\nprint('[Info] {} instances from train set'.format(len(train_src)))\nprint('[Info] {} instances from valid set'.format(len(valid_tgt)))\ntrain_loader = SCIterator(train_src, train_tgt, opt)\nvalid_loader = SCIterator(valid_src, valid_tgt, opt)\nprint(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:05.130244Z","iopub.execute_input":"2024-05-06T03:05:05.130670Z","iopub.status.idle":"2024-05-06T03:05:24.279142Z","shell.execute_reply.started":"2024-05-06T03:05:05.130635Z","shell.execute_reply":"2024-05-06T03:05:24.278079Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54a6d5d6adef4ac3ab263df9665ea9c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4839a86871b34745b2f59444c849a536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ddb5d7a14f4048874ae31a2b0ed977"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"[Info] 52595 instances from train set\n[Info] 2356 instances from valid set\n<torch.utils.data.dataloader.DataLoader object at 0x7eecd4970940>\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(5):\n    print(train_src[i])\n    print(train_tgt[i])\n    print(\"-\"*25)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:28.417083Z","iopub.execute_input":"2024-05-06T03:05:28.417946Z","iopub.status.idle":"2024-05-06T03:05:28.423207Z","shell.execute_reply.started":"2024-05-06T03:05:28.417917Z","shell.execute_reply":"2024-05-06T03:05:28.422307Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[8, 1974, 37, 86, 18, 3612, 210, 7, 59, 1776, 3, 9, 2297, 1974, 68, 6613, 11, 207, 55, 1]\n[37, 86, 18, 3612, 210, 7, 1974, 19, 29, 31, 17, 3, 9, 2297, 1974, 6, 68, 34, 31, 7, 8957, 5, 1]\n-------------------------\n[24, 543, 410, 59, 428, 140, 5931, 8115, 7, 599, 23, 317, 61, 1]\n[27, 278, 31, 17, 317, 24, 543, 1891, 140, 19601, 5, 1]\n-------------------------\n[13, 4301, 7, 15, 3, 23, 36, 3, 10674, 77, 34, 3, 15, 208, 651, 239, 6, 82, 3, 89, 9, 208, 3, 4059, 1836, 449, 19, 86, 76, 3198, 9, 1]\n[27, 1605, 34, 4604, 6, 82, 1305, 3, 4059, 1836, 449, 19, 86, 76, 3198, 9, 5, 1]\n-------------------------\n[661, 15, 12002, 5, 287, 41, 2258, 1082, 333, 34, 61, 3, 184, 694, 1939, 77, 5, 287, 41, 29117, 138, 61, 1]\n[9259, 1939, 77, 5, 287, 11, 661, 15, 12002, 5, 287, 33, 248, 21, 384, 694, 5, 1]\n-------------------------\n[27, 7, 3, 88, 16998, 58, 3845, 47, 30, 13294, 5190, 28, 1193, 152, 411, 31, 279, 3483, 11, 3, 88, 3776, 1134, 16998, 1]\n[216, 47, 30, 8, 13294, 5190, 504, 28, 1193, 152, 411, 31, 279, 3483, 11, 3776, 16998, 5, 1]\n-------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming 'train_loader' is your DataLoader object\nfor batch_idx, batch in enumerate(train_loader):\n    if batch_idx < 2:  # Print only the first three batches\n        print(f\"Batch {batch_idx}:\")\n        # Assuming the batch contains features and labels\n        features, labels = batch\n        print(\"Features:\")\n        print(features)\n        print(\"Labels:\")\n        print(labels)\n        print(\"-\" * 50)  # Print separator\n    else:\n        break  # Stop after printing the first three batches","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:31.312602Z","iopub.execute_input":"2024-05-06T03:05:31.312978Z","iopub.status.idle":"2024-05-06T03:05:31.471291Z","shell.execute_reply.started":"2024-05-06T03:05:31.312951Z","shell.execute_reply":"2024-05-06T03:05:31.470262Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Batch 0:\nFeatures:\ntensor([[  156,    34,    19,  ...,     1,     1,     1],\n        [   34,   373,  1330,  ...,     1,     1,     1],\n        [ 9034,  7105,    19,  ...,     1,     1,     1],\n        ...,\n        [   27,  1971,  3355,  ...,     1,     1,     1],\n        [  696,  2039,    19,  ...,     1,     1,     1],\n        [16497,    27,    54,  ...,     1,     1,     1]])\nLabels:\ntensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n        0, 0, 0, 0, 1, 0, 1, 0])\n--------------------------------------------------\nBatch 1:\nFeatures:\ntensor([[   27,   317,    25,   228,   169,  3306, 13601,    15,     5,   299,\n             3,    99,    24,   744,    31,    17,   161,     6,    25,    54,\n          3442,  3190,     7,    11,  2025,   135,    12,    39,  1218,     5,\n             1],\n        [ 1129,   503,     6, 18981,   138, 15343,     5,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [ 6276,    30,   160,   809,    57,     8,  3645,     3,     7,  3877,\n             7,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  150,   800,     3,    18,    12,   778,    16,     8,   183,    21,\n           140,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  466,    19,     3,     9,   418,    13,   223,  1583,    12,   474,\n           139,     3,     9,  3116,    13, 12792,     5,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  446,  2326,  6827,     3, 27262,    71,  6760, 15628,   427,  3087,\n         11927,  2344,   454,  3291,     3,  6299, 12335,  9191,  8229,   377,\n          9131,  7933, 18284,  3430,  5097, 10781,   454, 17131,  7896,  5946,\n             1],\n        [   27,   183,   270,   250,    13,  5704,    29,    63,  2556,     7,\n          3186,     5,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   27,   857,   132,   164,    36,     3,     9,  6722,    16,   132,\n             5,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [    8,   192,  3567,    24, 25117,   122,    33,  1312,    11,    25,\n           217,    70,  1245,     3,    17,  7085,     5,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  499,  1305, 15074,  2850,    33,  3059,    11, 16637,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [    3,   210,   302,     7,    25,   225,    36, 17101,    21,    44,\n           709,  9455,   716,   274,    25,   456,  6358,  6809,    29,  1014,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   94,    19,   914,   718,     3,     9,  9321,    68,     3,    23,\n           580,   135,  2039,  9321,     7,     5,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [    3,    23,  9689,    24,     3, 13863,    63,    56,   129,    91,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   27,   253,    34,  1134,  2879,   841,  1307,   641,  1380,    48,\n           822,     5,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [ 2163,   255,  4464,     8, 16998,   573,     5, 14125,    40,    32,\n            40,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   37,  2324,   797,    27,    26,    23,    32,    17,    47,  3032,\n            57,  1862,  1430,  2853,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [22070,   140,   116,    27,   497,    27,  1509,    24,  5640,     5,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [ 1615,   405,    34,    43,    12,    36,    80,    42,     8,   119,\n          6161,  8665,  8665,  3158,  8665,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   27,   261,    12,   114,   135,   116,    27,    47,  9264,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   39,  5722,   477,    33,     3,    89,    15,   115,     5,   519,\n            11,     3,    89,    15,   115,     5,  2517,     5,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   27,     7,    34,    16, 20576,     6,  2993,     3,  1436,  1496,\n            15,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  148,    54,  3332,    12,   960,    34,   367,     3,    18,    96,\n         23934,   428,   140,     8,   200,  1525,   979,  1137,   256,   163,\n            30,   593,   204,  4720,     1,     1,     1,     1,     1,     1,\n             1],\n        [    3,  8727, 23394,     3, 12860,    47,   394,   145,   272, 27472,\n            55,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   11,    79,    33,   341,  2145,    34,  2088,    17,   233,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  571,    81,   424,  2684,    24,    56,   474,    25,    16,     8,\n          6526,     5,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [ 1972,    91,    82,   415,   723,  1078,    65,    34,     5,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [    8,  4876,    33,    92,   207,    28,     8,  3605,     3,    29,\n            66,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   27,    31,    51,     3, 20305,     6,   437,  2715, 19650,    40,\n          1106,   164,    36,   396,   625,     5,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [   27,   183,  7918,   125,   255,    19,  2581,    21,     5,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  148,   225,    36,     3,   179,    12,   253,    34,    30,     8,\n           353,  2442,     5,     7,   152, 10309,   288,     9,     5,   287,\n            16,     8,  8426,  4876,  1375,     5,     1,     1,     1,     1,\n             1],\n        [ 2867, 22712, 11874,    57,   368,  7504,    11, 22712, 12574,    57,\n         13759,   314,  6644,    33,  5295,     5,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1],\n        [  863,   129,     3,     9,  1368,  1154,    11,   281,   719,  2570,\n         11632, 21820,  3320,    26,    60,     5,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1]])\nLabels:\ntensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n        1, 1, 0, 1, 1, 1, 1, 1])\n--------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"if os.path.exists(f'/kaggle/working/checkpoints/{opt.dataset}_embedding.pt'):\n        embedding = torch.load(f'./checkpoints/{opt.dataset}_embedding.pt')\nelse:\n    embed_path = '/kaggle/working/glove.840B.300d.txt'\n    embedding = load_embedding(tokenizer, 300, embed_path)\n    torch.save(embedding, f'/kaggle/working/{opt.dataset}_embedding.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:05:37.228896Z","iopub.execute_input":"2024-05-06T03:05:37.229277Z","iopub.status.idle":"2024-05-06T03:09:00.624636Z","shell.execute_reply.started":"2024-05-06T03:05:37.229245Z","shell.execute_reply":"2024-05-06T03:09:00.623720Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[Info] Loading embedding\n","output_type":"stream"},{"name":"stderr","text":"2024-05-06 03:08:31.546781: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 03:08:31.546894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 03:08:31.673610: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model = TextCNN(opt.embed_dim, len(tokenizer), filter_sizes, \n                    num_filters, embedding=embedding, dropout=opt.dropout)\nmodel.to(device).train()\n\noptimizer = ScheduledOptim(\n    torch.optim.Adam(filter(lambda x:x.requires_grad, model.parameters()),\n                     betas=(0.9, 0.98), eps=1e-09), opt.lr)\n\nloss_fn = nn.CrossEntropyLoss()\nprint('[Info] Built a model with {} parameters'.format(\n       sum(p.numel() for p in model.parameters())))\nprint('[Info]', opt)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:09:05.923022Z","iopub.execute_input":"2024-05-06T03:09:05.923694Z","iopub.status.idle":"2024-05-06T03:09:06.892148Z","shell.execute_reply.started":"2024-05-06T03:09:05.923646Z","shell.execute_reply":"2024-05-06T03:09:06.891171Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[Info] Built a model with 10412402 parameters\n[Info] Namespace(lr=0.001, dataset='em', embed_dim=300, seed=42, min_count=0, dropout=0.5, max_len=50, log_step=100, eval_step=1000, batch_size=32, epoch=50)\n","output_type":"stream"}]},{"cell_type":"code","source":"def main():\n    \n    c_path='/kaggle/working/checkpoints/t5_textcnn_{}.chkpt'.format(opt.dataset)\n    output_dir = os.path.dirname(c_path)\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Write to the file\n    with open(c_path, 'a') as file:\n        pass\n\n    tab = 0\n    avg_acc = 0\n    total_acc = 0.\n    total_num = 0.\n    total_loss = 0.\n    start = time.time()\n    for e in range(opt.epoch):\n        model.train()\n        for idx, batch in enumerate(train_loader):\n            x_batch, y_batch = map(lambda x: x.to(device), batch)\n            optimizer.zero_grad()\n            logits = model(x_batch)\n            loss = loss_fn(logits, y_batch)\n            total_loss += loss\n            loss.backward()\n            optimizer.step()\n\n            y_hat = logits.argmax(dim=-1)\n            same = [float(p == q) for p, q in zip(y_batch, y_hat)]\n            total_acc += sum(same)\n            total_num += len(y_batch)\n\n            if optimizer.steps % opt.log_step == 0:\n                lr = optimizer._optimizer.param_groups[0]['lr']\n#                 print('[Info] Epoch {:02d}-{:05d}: | average acc {:.4f}% | '\n#                     'average loss {:.4f} | lr {:.6f} | second {:.2f}'.format(\n#                     e, optimizer.steps, total_acc / total_num * 100,\n#                     total_loss / (total_num), lr, time.time() - start))\n                start = time.time()\n\n            if optimizer.steps % opt.eval_step == 0:\n                valid_acc, valid_loss = evaluate_sc(model, valid_loader, loss_fn, e)\n                if avg_acc < valid_acc:\n                    avg_acc = valid_acc\n                    save_path = '/kaggle/working/checkpoints/t5_textcnn_{}.chkpt'.format(opt.dataset)\n                    torch.save(model.state_dict(), save_path)\n                    print('[Info] The checkpoint file has been updated.')\n                    tab = 0\n                else:\n                    tab += 1\n                    if tab == 10:\n                        break\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:09:10.549272Z","iopub.execute_input":"2024-05-06T03:09:10.550031Z","iopub.status.idle":"2024-05-06T03:34:06.424710Z","shell.execute_reply.started":"2024-05-06T03:09:10.549999Z","shell.execute_reply":"2024-05-06T03:34:06.423645Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[Info] Epoch 00-valid: acc 90.9230% | loss 0.0066\n[Info] The checkpoint file has been updated.\n[Info] Epoch 00-valid: acc 91.6300% | loss 0.0061\n[Info] The checkpoint file has been updated.\n[Info] Epoch 00-valid: acc 92.4709% | loss 0.0058\n[Info] The checkpoint file has been updated.\n[Info] Epoch 01-valid: acc 92.6046% | loss 0.0057\n[Info] The checkpoint file has been updated.\n[Info] Epoch 01-valid: acc 90.7701% | loss 0.0070\n[Info] Epoch 01-valid: acc 92.4326% | loss 0.0056\n[Info] Epoch 02-valid: acc 92.7957% | loss 0.0058\n[Info] The checkpoint file has been updated.\n[Info] Epoch 02-valid: acc 92.5664% | loss 0.0057\n[Info] Epoch 02-valid: acc 92.0696% | loss 0.0061\n[Info] Epoch 03-valid: acc 92.3753% | loss 0.0063\n[Info] Epoch 03-valid: acc 93.1779% | loss 0.0054\n[Info] The checkpoint file has been updated.\n[Info] Epoch 03-valid: acc 92.6811% | loss 0.0059\n[Info] Epoch 03-valid: acc 92.9104% | loss 0.0057\n[Info] Epoch 04-valid: acc 93.1015% | loss 0.0060\n[Info] Epoch 04-valid: acc 92.8530% | loss 0.0060\n[Info] Epoch 04-valid: acc 92.8530% | loss 0.0061\n[Info] Epoch 05-valid: acc 92.6428% | loss 0.0066\n[Info] Epoch 05-valid: acc 92.8722% | loss 0.0072\n[Info] Epoch 05-valid: acc 92.8722% | loss 0.0067\n[Info] Epoch 06-valid: acc 92.5855% | loss 0.0073\n[Info] Epoch 06-valid: acc 92.6428% | loss 0.0071\n[Info] Epoch 07-valid: acc 92.9868% | loss 0.0071\n[Info] Epoch 07-valid: acc 92.6428% | loss 0.0075\n[Info] Epoch 07-valid: acc 92.9868% | loss 0.0067\n[Info] Epoch 08-valid: acc 93.1397% | loss 0.0071\n[Info] Epoch 08-valid: acc 92.9868% | loss 0.0073\n[Info] Epoch 08-valid: acc 92.7384% | loss 0.0075\n[Info] Epoch 09-valid: acc 93.1588% | loss 0.0075\n[Info] Epoch 09-valid: acc 92.9295% | loss 0.0075\n[Info] Epoch 09-valid: acc 92.8339% | loss 0.0077\n[Info] Epoch 10-valid: acc 93.0633% | loss 0.0074\n[Info] Epoch 10-valid: acc 92.7193% | loss 0.0080\n[Info] Epoch 10-valid: acc 92.8530% | loss 0.0081\n[Info] Epoch 10-valid: acc 92.9295% | loss 0.0078\n[Info] Epoch 11-valid: acc 92.6428% | loss 0.0081\n[Info] Epoch 11-valid: acc 92.9486% | loss 0.0077\n[Info] Epoch 11-valid: acc 92.8148% | loss 0.0082\n[Info] Epoch 12-valid: acc 92.8339% | loss 0.0083\n[Info] Epoch 12-valid: acc 92.9677% | loss 0.0082\n[Info] Epoch 12-valid: acc 92.8913% | loss 0.0083\n[Info] Epoch 13-valid: acc 92.8339% | loss 0.0082\n[Info] Epoch 13-valid: acc 92.9868% | loss 0.0082\n[Info] Epoch 13-valid: acc 92.8913% | loss 0.0084\n[Info] Epoch 13-valid: acc 92.8913% | loss 0.0083\n[Info] Epoch 14-valid: acc 92.9486% | loss 0.0083\n[Info] Epoch 14-valid: acc 92.8339% | loss 0.0085\n[Info] Epoch 14-valid: acc 92.9868% | loss 0.0083\n[Info] Epoch 15-valid: acc 92.8722% | loss 0.0085\n[Info] Epoch 15-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 15-valid: acc 92.8530% | loss 0.0085\n[Info] Epoch 16-valid: acc 92.7766% | loss 0.0087\n[Info] Epoch 16-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 16-valid: acc 92.9868% | loss 0.0084\n[Info] Epoch 17-valid: acc 93.0059% | loss 0.0084\n[Info] Epoch 17-valid: acc 92.8339% | loss 0.0086\n[Info] Epoch 17-valid: acc 92.9677% | loss 0.0085\n[Info] Epoch 17-valid: acc 92.9677% | loss 0.0085\n[Info] Epoch 18-valid: acc 92.9677% | loss 0.0086\n[Info] Epoch 18-valid: acc 92.8530% | loss 0.0086\n[Info] Epoch 18-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 19-valid: acc 92.8530% | loss 0.0086\n[Info] Epoch 19-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 19-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 20-valid: acc 92.8722% | loss 0.0085\n[Info] Epoch 20-valid: acc 92.8722% | loss 0.0086\n[Info] Epoch 20-valid: acc 92.8148% | loss 0.0086\n[Info] Epoch 20-valid: acc 92.8339% | loss 0.0086\n[Info] Epoch 21-valid: acc 92.8148% | loss 0.0086\n[Info] Epoch 21-valid: acc 92.8530% | loss 0.0085\n[Info] Epoch 21-valid: acc 92.8913% | loss 0.0085\n[Info] Epoch 22-valid: acc 92.8530% | loss 0.0086\n[Info] Epoch 22-valid: acc 92.8722% | loss 0.0085\n[Info] Epoch 22-valid: acc 92.8722% | loss 0.0086\n[Info] Epoch 23-valid: acc 92.8530% | loss 0.0086\n[Info] Epoch 23-valid: acc 92.8722% | loss 0.0085\n[Info] Epoch 23-valid: acc 92.8913% | loss 0.0086\n[Info] Epoch 24-valid: acc 92.8913% | loss 0.0087\n[Info] Epoch 24-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 24-valid: acc 92.9677% | loss 0.0085\n[Info] Epoch 24-valid: acc 92.8913% | loss 0.0086\n[Info] Epoch 25-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 25-valid: acc 92.8913% | loss 0.0086\n[Info] Epoch 25-valid: acc 92.9104% | loss 0.0086\n[Info] Epoch 26-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 26-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 26-valid: acc 92.8913% | loss 0.0086\n[Info] Epoch 27-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 27-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 27-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 27-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 28-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 28-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 28-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 29-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 29-valid: acc 92.8913% | loss 0.0085\n[Info] Epoch 29-valid: acc 92.8913% | loss 0.0085\n[Info] Epoch 30-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 30-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 30-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 31-valid: acc 92.9104% | loss 0.0086\n[Info] Epoch 31-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 31-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 31-valid: acc 92.9104% | loss 0.0086\n[Info] Epoch 32-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 32-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 32-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 33-valid: acc 92.8913% | loss 0.0086\n[Info] Epoch 33-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 33-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 34-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 34-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 34-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 34-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 35-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 35-valid: acc 92.8913% | loss 0.0085\n[Info] Epoch 35-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 36-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 36-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 36-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 37-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 37-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 37-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 38-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 38-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 38-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 38-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 39-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 39-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 39-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 40-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 40-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 40-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 41-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 41-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 41-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 41-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 42-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 42-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 42-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 43-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 43-valid: acc 92.9104% | loss 0.0086\n[Info] Epoch 43-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 44-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 44-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 44-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 45-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 45-valid: acc 92.9295% | loss 0.0085\n[Info] Epoch 45-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 45-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 46-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 46-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 46-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 47-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 47-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 47-valid: acc 92.9104% | loss 0.0085\n[Info] Epoch 48-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 48-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 48-valid: acc 92.9486% | loss 0.0086\n[Info] Epoch 48-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 49-valid: acc 92.9295% | loss 0.0086\n[Info] Epoch 49-valid: acc 92.9486% | loss 0.0085\n[Info] Epoch 49-valid: acc 92.9295% | loss 0.0085\n","output_type":"stream"}]},{"cell_type":"code","source":"#for testing the text cnn classifier \nfilter_sizes = [1, 2, 3, 4, 5]\nnum_filters = [128, 128, 128, 128, 128]\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nspecial_tokens = [{'bos_token': '<bos>'},\n                  {'eos_token': '<eos>'}, {'sep_token': '<sep>'},\n                  {'pad_token': '<pad>'}, {'unk_token': '<unk>'}]\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\ndef main():\n    parser = argparse.ArgumentParser('Evaluating Style Strength')\n    parser.add_argument('-order', default=0, type=str, help='order')\n    parser.add_argument('-style', default=0, type=int, help='from 0 to 1')\n    parser.add_argument('-max_len', default=50, type=int, help='max tokens in a batch')\n    parser.add_argument('-embed_dim', default=300, type=int, help='the embedding size')\n    parser.add_argument('-dataset', default='em', type=str, help='the name of dataset')\n    parser.add_argument('-model', default='textcnn', type=str, help='the name of model')\n    parser.add_argument('-seed', default=42, type=int, help='pseudo random number seed')\n    parser.add_argument('-batch_size', default=32, type=int, help='max sents in a batch')\n    parser.add_argument(\"-dropout\", default=0.5, type=float, help=\"Keep prob in dropout\")\n    parser.add_argument(\"-file\",default=\"/kaggle/input/dataproject/data/fr/test/formal\", type=str)\n    opt = parser.parse_args([])\n    torch.manual_seed(opt.seed)\n\n    test_src, test_tgt = [], []\n\n    if opt.file is not None:\n        with open(opt.file, 'r') as f:\n            for line in f.readlines():\n                test_tgt.append(tokenizer.encode(line.strip())[:opt.max_len])\n\n    print('[Info] {} instances from src test set'.format(len(test_src)))\n    print('[Info] {} instances from tgt test set'.format(len(test_tgt)))\n    test_loader = SCIterator(test_src, test_tgt, opt, tokenizer.pad_token_id, shuffle=False)\n\n    loss_fn = nn.CrossEntropyLoss()\n    model = TextCNN(opt.embed_dim, len(tokenizer), filter_sizes,\n                    num_filters, None, dropout=opt.dropout)\n    model.to(device).eval()\n    model.load_state_dict(torch.load('/kaggle/working/checkpoints/t5_textcnn_{}.chkpt'.format(\n        opt.dataset)))\n\n    total_num = 0.\n    total_acc = 0.\n    total_loss = 0.\n    with torch.no_grad():\n        for i,batch in enumerate(test_loader):\n            x_batch, y_batch = map(lambda x: x.to(device), batch)\n            logits = model(x_batch)\n            F.softmax(logits, dim=-1)\n            total_loss += loss_fn(logits, y_batch)\n            _, y_hat = torch.max(logits,dim=-1)\n            same = [float(p == q) for p, q in zip(y_batch, y_hat)]\n            total_acc += sum(same)\n            total_num += len(y_batch)\n#             print(i,\"done\")\n\n    print('Test: {}'.format('acc {:.4f}% | loss {:.4f}').format(\n        (total_acc / total_num) * 100, total_loss / total_num))\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:35:30.338088Z","iopub.execute_input":"2024-05-06T03:35:30.338505Z","iopub.status.idle":"2024-05-06T03:35:32.176747Z","shell.execute_reply.started":"2024-05-06T03:35:30.338474Z","shell.execute_reply":"2024-05-06T03:35:32.175597Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"[Info] 0 instances from src test set\n[Info] 1019 instances from tgt test set\nTest: acc 96.6634% | loss 0.0028\n","output_type":"stream"}]},{"cell_type":"code","source":"def optimize(opt, loss, retain_graph=False):\n    opt.zero_grad()\n    loss.backward(retain_graph=retain_graph)\n    opt.step()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:35:36.423616Z","iopub.execute_input":"2024-05-06T03:35:36.424615Z","iopub.status.idle":"2024-05-06T03:35:36.429667Z","shell.execute_reply.started":"2024-05-06T03:35:36.424576Z","shell.execute_reply":"2024-05-06T03:35:36.428744Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class T5Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, src_file, tgt_file, tokenizer, max_len):\n\n        with open(src_file, 'r') as f1, open(tgt_file,'r') as f2:\n            self.src = f1.readlines()\n            self.tgt = f2.readlines()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, index):\n        ctext = self.src[index].strip()\n        ctext = ' '.join(ctext.split())\n\n        text = self.tgt[index].strip()\n        text = ' '.join(text.split())\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.max_len, padding='max_length',truncation=True,return_tensors='pt')\n        target = self.tokenizer.batch_encode_plus([text], max_length= self.max_len, padding='max_length',truncation=True,return_tensors='pt')\n        \n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long),\n            'source_mask': source_mask.to(dtype=torch.long),\n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:35:38.552443Z","iopub.execute_input":"2024-05-06T03:35:38.552847Z","iopub.status.idle":"2024-05-06T03:35:38.564304Z","shell.execute_reply.started":"2024-05-06T03:35:38.552817Z","shell.execute_reply":"2024-05-06T03:35:38.563308Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class T5UnsupDataset(torch.utils.data.Dataset):\n\n    def __init__(self, src_file, aug_file, tokenizer, max_len):\n\n        with open(src_file, 'r') as f1, open(aug_file,'r') as f2:\n            self.src = f1.readlines()\n            self.aug = f2.readlines()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, index):\n        ctext = self.src[index].strip()\n        ctext = ' '.join(ctext.split())\n\n        text = self.aug[index].strip()\n        text = ' '.join(text.split())\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.max_len, padding='max_length',truncation=True,return_tensors='pt')\n        augment = self.tokenizer.batch_encode_plus([text], max_length= self.max_len, padding='max_length',truncation=True,return_tensors='pt')\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        augment_ids = augment['input_ids'].squeeze()\n        augment_mask = augment['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long),\n            'source_mask': source_mask.to(dtype=torch.long),\n            'augment_ids': augment_ids.to(dtype=torch.long),\n            'augment_mask': augment_mask.to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-05-06T04:33:28.637575Z","iopub.execute_input":"2024-05-06T04:33:28.638019Z","iopub.status.idle":"2024-05-06T04:33:28.652052Z","shell.execute_reply.started":"2024-05-06T04:33:28.637985Z","shell.execute_reply":"2024-05-06T04:33:28.650841Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"class T5AugDataset(torch.utils.data.Dataset):\n\n    def __init__(self, src_file, augmentor, tokenizer, max_len, aug_p=0.1, dataset='em'):\n\n        with open(src_file, 'r') as f1:\n            self.src = f1.readlines()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.augmentor = augment_choice(augmentor, aug_p, dataset=dataset)\n        if self.augmentor is None:\n            self.aug = False\n        else:\n            self.aug = True\n        if isinstance(self.augmentor, list):\n            self.mix = True\n        else:\n            self.mix = False\n\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, index):\n        ctext = self.src[index].strip()\n        ctext = ' '.join(ctext.split())\n\n        if self.aug:\n            if self.mix:\n                augmentor = random.choice(self.augmentor)\n            else:\n                augmentor = self.augmentor\n\n            text = augmentor.augment(ctext)\n        else:\n            text = ctext\n\n        text = ' '.join(text.split())\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.max_len, padding='max_length',truncation=True,return_tensors='pt')\n        augment = self.tokenizer.batch_encode_plus([text], max_length= self.max_len, padding='max_length',truncation=True,return_tensors='pt')\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        augment_ids = augment['input_ids'].squeeze()\n        augment_mask = augment['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long),\n            'source_mask': source_mask.to(dtype=torch.long),\n            'augment_ids': augment_ids.to(dtype=torch.long),\n            'augment_mask': augment_mask.to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:35:40.999193Z","iopub.execute_input":"2024-05-06T03:35:40.999934Z","iopub.status.idle":"2024-05-06T03:35:41.012348Z","shell.execute_reply.started":"2024-05-06T03:35:40.999901Z","shell.execute_reply":"2024-05-06T03:35:41.011225Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu,sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nimport nltk\ndef bleu(reference_files_src_list,gen_file_src,ngrams=4,ignore_case=False):\n    all_reference=[]\n    for src in reference_files_src_list:\n        with open(src,'r',encoding='utf-8') as f:\n            one_reference=[]\n            for line in f:\n                if not ignore_case:\n                    one_reference.append(nltk.word_tokenize(line.strip()))\n                else:\n                    one_reference.append(nltk.word_tokenize(line.strip().lower()))\n            all_reference.append(one_reference)\n    all_reference=[[all_reference[i][j] for i in range(0,len(all_reference))] for j in range(0,len(all_reference[0]))]\n    gen=[]\n    with open(gen_file_src,'r',encoding='utf-8') as f:\n        for line in f:\n            if not ignore_case:\n                gen.append(nltk.word_tokenize(line.strip()))\n            else:\n                gen.append(nltk.word_tokenize(line.strip().lower()))\n    weight=[1.0/ngrams]*ngrams\n    # print(len(gen))\n    b=corpus_bleu(all_reference,gen,weights=weight)\n    return b\n\n\ndef get_ref_src_list(path_prefix,ref_num=4):\n    src_list=[]\n    for i in range(0,ref_num):\n        src_list.append(path_prefix+str(i))\n    return src_list\n\n\ndef evaluate_bleu(ref_path, pred_path):\n    # def eval_factory(log_dict,re):\n    re = [ref_path + \".ref{}\".format(t) for t in range(4)]\n\n    return bleu(re, pred_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:35:42.194550Z","iopub.execute_input":"2024-05-06T03:35:42.195401Z","iopub.status.idle":"2024-05-06T03:35:42.207258Z","shell.execute_reply.started":"2024-05-06T03:35:42.195368Z","shell.execute_reply":"2024-05-06T03:35:42.206146Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\nimport os\nimport time\nimport argparse\nimport numpy as np\nimport nlpaug.augmenter.word as naw\nimport nltk\nimport random\n\n\nimport torch\nfrom torch import cuda\n\n# from utils.T5_dataset import T5Dataset\nfrom torch.utils.data import DataLoader\n# from utils.dataset import  SCIterator\n# from utils.nltk_bleu import evaluate_bleu\n\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\nfilter_sizes = [1, 2, 3, 4, 5]\nnum_filters = [128, 128, 128, 128, 128]\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    torch.cuda.manual_seed_all(seed)\n\n\ndef test(model, tokenizer, cls, cls_tokenizer, opt):\n\n    styles = ['informal', 'formal']\n\n    test_src_file = '/kaggle/input/dataproject/data/{}/{}/{}'.format(opt.dataset, 'test', styles[opt.style])\n\n    test_tgt_file = '/kaggle/input/dataproject/data/{}/{}/{}.ref0'.format(opt.dataset, 'test', styles[1 - opt.style])\n\n    test_label_files = f'/kaggle/input/dataproject/data/{opt.dataset}/test/{styles[1 - opt.style]}'\n\n    test_dataset = T5Dataset(test_src_file, test_tgt_file, tokenizer, opt.max_len)\n    test_loader = DataLoader(test_dataset,\n                             num_workers=2,\n                             batch_size=opt.val_batch_size,\n                             shuffle=False)\n\n    print('[Info] {} instances from test set'.format(len(test_dataset)))\n\n    print(\"Test starts...\")\n\n    model.eval()\n\n\n    start = time.time()\n    pred_list = []\n\n    if not os.path.exists(f'./data/{opt.dataset}/outputs/{opt.model}/'):\n        os.mkdir(f'./data/{opt.dataset}/outputs/{opt.model}/')\n    with open('./data/{}/outputs/{}/{}_{}_{}.{}_best_test.txt'.format(opt.dataset, opt.model,\n                                                                      opt.model, opt.dataset, opt.order, opt.style),\n              'w') as fout:\n        for idx, data in enumerate(test_loader):\n            if idx % 10 == 0:\n                print('[Info] processing {} batches | seconds {:.4f}'.format(\n                    idx, time.time() - start))\n                start = time.time()\n\n            ids = data['source_ids'].to(device, dtype=torch.long)\n            mask = data['source_mask'].to(device, dtype=torch.long)\n\n            generated_ids = model.generate(ids,\n                                           attention_mask=mask,\n                                           num_beams=5,\n                                           max_length=30)\n\n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in\n                     generated_ids]\n\n            pred_list.extend(preds)\n        for text in pred_list:\n            fout.write(text.strip() + '\\n')\n\n    model.train()\n\n    pred_file = './data/{}/outputs/{}/{}_{}_{}.{}_best_test.txt'.format(opt.dataset, opt.model,\n                                                                        opt.model, opt.dataset, opt.order, opt.style)\n\n    bleu = evaluate_bleu(test_label_files, pred_file)\n    print(bleu)\n\n    test_tgt = []\n    test_src = []\n    with open(pred_file, 'r') as f:\n        for line in f.readlines():\n            if opt.style == 0:\n                test_tgt.append(cls_tokenizer.encode(line.strip())[:opt.max_len])\n            else:\n                test_src.append(cls_tokenizer.encode(line.strip())[:opt.max_len])\n    cls_loader = SCIterator(test_src, test_tgt, opt, cls_tokenizer.pad_token_id)\n    cls_loss_fn = torch.nn.CrossEntropyLoss()\n\n    total_num = 0.\n    total_acc = 0.\n    total_loss = 0\n    with torch.no_grad():\n        for i, batch in enumerate(cls_loader):\n            x_batch, y_batch = map(lambda x: x.to(device), batch)\n            logits = cls(x_batch)\n            # print(F.softmax(logits, dim=-1))\n            total_loss += cls_loss_fn(logits, y_batch)\n            _, y_hat = torch.max(logits, dim=-1)\n            same = [float(p == q) for p, q in zip(y_batch, y_hat)]\n            total_acc += sum(same)\n            total_num += len(y_batch)\n\n    print('Test: {}'.format('acc {:.4f}% | loss {:.4f}').format(\n        total_acc / total_num * 100, total_loss / total_num))\n\n    with open('./data/{}/outputs/{}/{}_{}_{}_bleu_test.txt'.format(opt.dataset, opt.model,\n                                                                   opt.model, opt.dataset, opt.order, opt.style),\n              'a') as fbl:\n        fbl.write(\n            'Test Bleu score for model {}: {:.4f};  Acc: {:.4f}\\n'.format(opt.order, bleu, total_acc / total_num * 100))\n\n    return bleu, total_acc / total_num * 100, total_loss / total_num","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:35:43.300193Z","iopub.execute_input":"2024-05-06T03:35:43.300855Z","iopub.status.idle":"2024-05-06T03:36:16.273371Z","shell.execute_reply.started":"2024-05-06T03:35:43.300824Z","shell.execute_reply":"2024-05-06T03:36:16.272489Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:36:22.804598Z","iopub.execute_input":"2024-05-06T03:36:22.804966Z","iopub.status.idle":"2024-05-06T03:36:22.814156Z","shell.execute_reply.started":"2024-05-06T03:36:22.804940Z","shell.execute_reply":"2024-05-06T03:36:22.813189Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer\nimport os\nimport time\nimport argparse\nimport random, re, math\n\n\n\nimport torch\nfrom torch import cuda\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom pyskiplist import SkipList\nimport fitlog\nimport kenlm\nimport statistics\nfrom torch.nn import CrossEntropyLoss","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:36:27.129037Z","iopub.execute_input":"2024-05-06T03:36:27.129466Z","iopub.status.idle":"2024-05-06T03:36:27.145894Z","shell.execute_reply.started":"2024-05-06T03:36:27.129433Z","shell.execute_reply":"2024-05-06T03:36:27.144828Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\n# parameters for textCNN\nfilter_sizes = [1, 2, 3, 4, 5]\nnum_filters = [128, 128, 128, 128, 128]\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef prepare_batch_cls(insts, pad_token_id=1):\n    ''' Pad the instance to the max seq length in batch for CNN classifier '''\n\n    max_len = max(len(inst) for inst in insts)\n    max_len = max_len if max_len > 4 else 5\n\n    batch_seq = np.array([\n        inst + [pad_token_id] * (max_len - len(inst))\n        for inst in insts])\n    batch_seq = torch.LongTensor(batch_seq)\n\n    return batch_seq\n\ndef score_generated_sentences(generated_text_file_path, model):\n    #get perplexity scores for evaluation\n    log_probs = list()\n    perplexity_scores = list()\n\n    with open(generated_text_file_path) as generated_text_file:\n        for sentence in generated_text_file:\n            cleaned_sentence = clean_text(sentence)\n            log_probs.append(model.score(cleaned_sentence))\n            perplexity_scores.append(model.perplexity(cleaned_sentence))\n\n    return statistics.mean(log_probs), statistics.mean(perplexity_scores)\n\n\ndef clean_text(string):\n    string = string.replace(\".\", \"\")\n    string = string.replace(\".\", \"\")\n    string = string.replace(\"\\n\", \" \")\n    string = string.replace(\" 's\", \" is\")\n    string = string.replace(\"'m\", \" am\")\n    string = string.replace(\"'ve\", \" have\")\n    string = string.replace(\"n't\", \" not\")\n    string = string.replace(\"'re\", \" are\")\n    string = string.replace(\"'d\", \" would\")\n    string = string.replace(\"'ll\", \" will\")\n    string = string.replace(\"\\r\", \" \")\n    string = string.replace(\"\\n\", \" \")\n    string = re.sub(r'\\d+', \"number\", string)\n    string = ''.join(x for x in string if x.isalnum() or x == \" \")\n    string = re.sub(r'\\s{2,}', \" \", string)\n    string = string.strip().lower()\n\n    return string\n\n\ndef score_sentence(sentences, model):\n    # log_probs = list()\n    perplexity_scores = list()\n\n    for sentence in sentences:\n        cleaned_sentence = clean_text(sentence)\n        # log_probs.append(model.score(cleaned_sentence))\n        perplexity_scores.append(model.perplexity(cleaned_sentence))\n\n    return perplexity_scores\n\ndef sentence_bleu_score(sents, refs, ngrams=3):\n    sents = [nltk.word_tokenize(sent) for sent in sents]\n    refs = [nltk.word_tokenize(ref) for ref in refs]\n    weight=[1.0 / ngrams] * ngrams\n    scores = []\n    for sent, ref in zip(sents, refs):\n        scores.append(sentence_bleu([ref], sent, weights=weight))\n    return scores\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:36:28.838516Z","iopub.execute_input":"2024-05-06T03:36:28.839543Z","iopub.status.idle":"2024-05-06T03:36:28.858777Z","shell.execute_reply.started":"2024-05-06T03:36:28.839494Z","shell.execute_reply":"2024-05-06T03:36:28.857657Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# -style 0 \\\n#     -dataset em \\\n#     -order em.sup \\\n#     -batch_size 8 \\\n#     -val_batch_size 16 \\\n#     -lr 2e-5\n# -unsup \\\n#     -style 0 \\\n#     -ratio 1.0 \\\n#     -dataset em \\\n#     -order em-semi \\\n#     -pre_step 2000 \\\n#     -batch_size 8 \\\n#     -unsup_batch_size 56 \\\n#     -val_batch_size 16 \\\n#     -lr 2e-5  \\\n#     -aug_choice spell \\\n#     -aug_p 0.1  \\\n#     -filter cls \\\n#     -phi 0.4  \\\n#     -n_step 1000 \\\n#     -sigma 0.8\nparser = argparse.ArgumentParser('Fine-Tuned T5 for style transfer')\nparser.add_argument('-order', default=\"em-semi\", type=str, help='the order of traing')\nparser.add_argument('-style', default=0, type=int, help='transfer inf. to for.')\nparser.add_argument('-lr', default=2e-5, type=float, help='the learning rate')\nparser.add_argument('-ratio', default=1., type=float, help='proportion of data')\nparser.add_argument('-model', default='t5', type=str, help='the name of model')\nparser.add_argument('-model_name', default='t5-large', type=str, help='the name of model')\nparser.add_argument('-dataset', default='em', type=str, help='the name of dataset')\nparser.add_argument('-steps', default=30000, type=int, help='force stop at x steps')\nparser.add_argument('-batch_size', default=8, type=int, help='the size in a batch')\nparser.add_argument('-val_batch_size', default=16, type=int, help='the size in a batch')\nparser.add_argument('-max_len', default=50, type=int, help='maximum tokens a batch')\nparser.add_argument('-dropout', default=0.5, type=float, help='Keep prob in dropout')\nparser.add_argument('-patience', default=10, type=int, help='early stopping fine-tune')\nparser.add_argument('-seed', default=42, type=int, help='pseudo random generator seed')\nparser.add_argument('-log_step', default=100, type=int, help='print logs every x step')\nparser.add_argument('-eval_step', default=1000, type=int, help='evaluate every x step')\nparser.add_argument('-unsup', action='store_true', help='use unsupervised loss')\nparser.add_argument('-unsup_batch_size', default=56, type=int, help='batch size for unlabeled data')\nparser.add_argument('-weight', default=1.0, type=float, help='balance weight of unsup loss')\nparser.add_argument('-pre_step', default=2000, type=int, help='pretrain steps')\nparser.add_argument('-aug_type', default='real-time',type=str, help=\"whether to augment sentences while training\")\nparser.add_argument('-aug_choice', default='spell', type=str)\nparser.add_argument('-aug_p', default=0.1, type=float, help='augmentation probability')\nparser.add_argument('-filter', default='cls', type=str, help='metric used for data filtering')\nparser.add_argument('-phi', default=0.4, type=float, help='threshold for formality evaluation (cls filter)')\nparser.add_argument('-n_step', default=1000, type=int, help='number of steps for computing initial score list')\nparser.add_argument('-sigma', default=0.8, type=float, help='dynamic threshold for lm/bleu filtering')\nparser.add_argument('-ngrams', default=4, type=int)\nparser.add_argument('-log_dir', default='./logs', type=str, help='directory of logs')\n\n\nopt = parser.parse_args()\nprint('[Info]', opt)\nopt.unsup=False\nset_seed(opt.seed)\n# fitlog.debug() disenables fitlog, comment it if you want to use fitlog\nfitlog.debug()\nfitlog.set_log_dir(opt.log_dir)\nfitlog.add_hyper(opt)\nfitlog.add_other({\"notebook_name\": \"Nlpproject_123.ipynb\"})  # Manually specify notebook name\nfitlog.add_other({\"experiment_description\": \"Description of your experiment\"})\npath_now='./data/{}/outputs/{}/{}_{}_{}.{}_bleu.txt'.format(opt.dataset, opt.model,\n                                                             opt.model, opt.dataset, opt.order, opt.style)\noutput_dir = os.path.dirname(path_now)\nos.makedirs(output_dir, exist_ok=True)\nwith open(path_now, 'a') as file:\n    pass\nwith open('./data/{}/outputs/{}/{}_{}_{}.{}_bleu.txt'.format(opt.dataset, opt.model,\n                                                             opt.model, opt.dataset, opt.order, opt.style),\n          'a') as fbl:\n    fbl.write(str(opt) + '\\n')\n\nset_seed(opt.seed)\nfitlog.debug()\nfitlog.set_log_dir(opt.log_dir)\nfitlog.add_hyper(opt)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:36:33.430547Z","iopub.execute_input":"2024-05-06T03:36:33.431145Z","iopub.status.idle":"2024-05-06T03:36:33.457340Z","shell.execute_reply.started":"2024-05-06T03:36:33.431112Z","shell.execute_reply":"2024-05-06T03:36:33.456260Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"[Info] Namespace(order='em-semi', style=0, lr=2e-05, ratio=1.0, model='t5', model_name='t5-large', dataset='em', steps=30000, batch_size=8, val_batch_size=16, max_len=50, dropout=0.5, patience=10, seed=42, log_step=100, eval_step=1000, unsup=False, unsup_batch_size=56, weight=1.0, pre_step=2000, aug_type='real-time', aug_choice='spell', aug_p=0.1, filter='/root/.local/share/jupyter/runtime/kernel-a8a35ee9-a7cc-4b9b-9b2a-6916491ed99a.json', phi=0.4, n_step=1000, sigma=0.8, ngrams=4, log_dir='./logs')\n","output_type":"stream"}]},{"cell_type":"code","source":"# to train the model\ntokenizer = T5Tokenizer.from_pretrained(opt.model_name)\ncls_tokenizer = tokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained(opt.model_name)\n\nmodel.to(device).train()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:36:38.164192Z","iopub.execute_input":"2024-05-06T03:36:38.164930Z","iopub.status.idle":"2024-05-06T03:39:19.903759Z","shell.execute_reply.started":"2024-05-06T03:36:38.164896Z","shell.execute_reply":"2024-05-06T03:39:19.902720Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ea56a711eb43ed9418f33009591106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1626bcb56864901a461e04b96cdb214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dab9f3e78a14e748c6ead70a1ca8a80"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430ed6274fc54aa6a1b342701802e4d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c98b09701e34f0eb0d44540c88dc1e0"}},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"opt.unsup=True","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:25.489111Z","iopub.execute_input":"2024-05-06T03:39:25.489895Z","iopub.status.idle":"2024-05-06T03:39:25.494278Z","shell.execute_reply.started":"2024-05-06T03:39:25.489863Z","shell.execute_reply":"2024-05-06T03:39:25.493134Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# CNN classifier for evaluation\ncls = TextCNN(300, len(cls_tokenizer), filter_sizes,num_filters, None, dropout=opt.dropout)\ncls.to(device).eval()\ncls.load_state_dict(torch.load('./checkpoints/t5_textcnn_{}.chkpt'.format(opt.dataset)))\n\n\nstyles = ['informal', 'formal']\nif opt.style == 0:\n    unsup_file = f\"data/unlabeled/{opt.dataset.upper()}_200k_inf.txt\"\n    unsup_file = f\"/kaggle/input/dataproject/data/unlabeled/informal-e.txt\"\nelse:\n    raise ValueError(\"Invalid style.\")\n\ntrain_src_file = '/kaggle/input/dataproject/data/{}/{}/{}'.format(opt.dataset, 'train', styles[opt.style])\ntrain_tgt_file = '/kaggle/input/dataproject/data/{}/{}/{}'.format(opt.dataset, 'train', styles[1 - opt.style])\ntrain_dataset = T5Dataset(train_src_file, train_tgt_file, tokenizer, opt.max_len)\ntrain_loader = DataLoader(train_dataset,num_workers=2,batch_size=opt.batch_size,shuffle=True)\n\nval_src_file = '/kaggle/input/dataproject/data/{}/{}/{}'.format(opt.dataset, 'tune', styles[opt.style])\nval_tgt_file = '/kaggle/input/dataproject/data/{}/{}/{}.ref0'.format(opt.dataset, 'tune', styles[1 - opt.style])\nval_label_files = f'/kaggle/input/dataproject/data/{opt.dataset}/tune/{styles[1 - opt.style]}'\nval_dataset = T5Dataset(val_src_file, val_tgt_file, tokenizer, opt.max_len)\nval_loader = DataLoader(val_dataset,\n                        num_workers=2,\n                        batch_size=opt.val_batch_size,\n                        shuffle=False)\n\n# test_src_file = 'data/{}/{}/{}'.format(opt.dataset, 'test', styles[opt.style])\n# test_tgt_file = 'data/{}/{}/{}.ref0'.format(opt.dataset, 'test', styles[1 - opt.style])\n# test_label_files = f'data/{opt.dataset}/test/{styles[1 - opt.style]}'\n\n\nprint('[Info] {} instances from train set'.format(len(train_dataset)))\nprint('[Info] {} instances from validation set'.format(len(val_dataset)))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:29.895552Z","iopub.execute_input":"2024-05-06T03:39:29.896442Z","iopub.status.idle":"2024-05-06T03:39:30.188483Z","shell.execute_reply.started":"2024-05-06T03:39:29.896409Z","shell.execute_reply":"2024-05-06T03:39:30.187447Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"[Info] 52595 instances from train set\n[Info] 2877 instances from validation set\n","output_type":"stream"}]},{"cell_type":"code","source":"for idx,data in enumerate(train_dataset):\n    if idx<1:\n        print(data)\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:32.870182Z","iopub.execute_input":"2024-05-06T03:39:32.870851Z","iopub.status.idle":"2024-05-06T03:39:32.880020Z","shell.execute_reply.started":"2024-05-06T03:39:32.870819Z","shell.execute_reply":"2024-05-06T03:39:32.879007Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"{'source_ids': tensor([   8, 1974,   37,   86,   18, 3612,  210,    7,   59, 1776,    3,    9,\n        2297, 1974,   68, 6613,   11,  207,   55,    1,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0]), 'source_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'target_ids': tensor([  37,   86,   18, 3612,  210,    7, 1974,   19,   29,   31,   17,    3,\n           9, 2297, 1974,    6,   68,   34,   31,    7, 8957,    5,    1,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0]), 'target_ids_y': tensor([  37,   86,   18, 3612,  210,    7, 1974,   19,   29,   31,   17,    3,\n           9, 2297, 1974,    6,   68,   34,   31,    7, 8957,    5,    1,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0])}\n","output_type":"stream"}]},{"cell_type":"code","source":"for idx,data in enumerate(train_loader):\n    if idx<1:\n        print(data)\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:36.294066Z","iopub.execute_input":"2024-05-06T03:39:36.294748Z","iopub.status.idle":"2024-05-06T03:39:36.775962Z","shell.execute_reply.started":"2024-05-06T03:39:36.294717Z","shell.execute_reply":"2024-05-06T03:39:36.774661Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"{'source_ids': tensor([[    3,    10,    61,   446,    87,   157,    27,    31,    26,   987,\n            34,   541,    11,   278,    31,    17,   752,    34,   129,    48,\n           625,     5,     2,     1,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  216,   429,    36,     6,    27,  2483,   214,     5,     1,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   66,     8,  3605,     3,    88,   143,     3,    52,   836,  6417,\n             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [    3,     2,  1024,  4244,    46,    31,    17,    24,  7683,   202,\n            29,    63,     3,    23,   214,     2,   184,    88,  1408,     7,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   34,    56,   320,   114,     8,   490,   589,     5,     1,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [    3,   107,   173,  1208,     3,    26,  2999,     6,   841,    31,\n             7,  3355,   147,   140,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  242,   128,  1053,    27,   317,   255,    31,     7,     3, 13366,\n           281,    28,  3038,   308,   864,  2258,     5,     1,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   41, 17068,   596,     6,   666,     8,   600,  2812,     7,    61,\n          1429, 20639, 25408,  1935,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'source_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0]]), 'target_ids': tensor([[ 1142,   250,    27,  1380,     8,   822,   541,   405,    59,  1243,\n            34,  2347,   625,     5,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  216,   164,    36,     6,    27,   278,    31,    17,   214,     5,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  432,    13,   112,  4183,  3409,     7,    33,  1627,     5,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   27,   183,  2718,    24,    34,    19,    59,   902,  6613,     5,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  366,    34,    19,   612,     6,    34,    56,   320,   114,     8,\n           490,   589,     5,     1,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [20855,   309,  2999,     6, 16471,    31,     7,  4195,    53,  2035,\n          1212,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   27,   317,   255,    56,   854,  3038,   308,   864,  2258,     5,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  461,     8,   646,   596,     6,   666,     8,   600,  2812,     7,\n             5,  1804,  5851,    55,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'target_ids_y': tensor([[ 1142,   250,    27,  1380,     8,   822,   541,   405,    59,  1243,\n            34,  2347,   625,     5,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  216,   164,    36,     6,    27,   278,    31,    17,   214,     5,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  432,    13,   112,  4183,  3409,     7,    33,  1627,     5,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   27,   183,  2718,    24,    34,    19,    59,   902,  6613,     5,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  366,    34,    19,   612,     6,    34,    56,   320,   114,     8,\n           490,   589,     5,     1,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [20855,   309,  2999,     6, 16471,    31,     7,  4195,    53,  2035,\n          1212,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   27,   317,   255,    56,   854,  3038,   308,   864,  2258,     5,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [  461,     8,   646,   596,     6,   666,     8,   600,  2812,     7,\n             5,  1804,  5851,    55,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"# pretrained lm model for lm filter\nlanguage_model_path = f'/kaggle/input/checkpoint/checkpoints/{opt.dataset}_{styles[1-opt.style]}.arpa'\nlm_model = kenlm.Model(language_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:42.365030Z","iopub.execute_input":"2024-05-06T03:39:42.366089Z","iopub.status.idle":"2024-05-06T03:39:43.299967Z","shell.execute_reply.started":"2024-05-06T03:39:42.366051Z","shell.execute_reply":"2024-05-06T03:39:43.298729Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Loading the LM will be faster if you build a binary file.\nReading /kaggle/input/checkpoint/checkpoints/em_formal.arpa\n----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n****************************************************************************************************\n","output_type":"stream"}]},{"cell_type":"code","source":"print(opt.unsup)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:45.236796Z","iopub.execute_input":"2024-05-06T03:39:45.237824Z","iopub.status.idle":"2024-05-06T03:39:45.243186Z","shell.execute_reply.started":"2024-05-06T03:39:45.237777Z","shell.execute_reply":"2024-05-06T03:39:45.242100Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"def augment_choice(arg, aug_p, dataset='em'):\n    if arg == \"drop\":\n        return naw.random.RandomWordAug(action='delete', aug_p=aug_p)\n    elif arg == 'swap':\n        return naw.random.RandomWordAug(action='swap', aug_p=aug_p)\n    elif arg == 'unk':\n        return naw.random.RandomWordAug(action='substitute', aug_p=aug_p, target_words=['_'])\n    elif arg == 'synonym':\n        return naw.SynonymAug(aug_src='wordnet', aug_p=aug_p)\n    elif arg == 'spell':\n        # If using official Nlpaug, randomness will happen even with fixed random seeds.\n        return naw.SpellingAug(aug_p=aug_p)\n    elif arg == 'keyboard':\n        return nac.KeyboardAug(aug_word_p=aug_p)\n    elif arg == 'tfidf':\n        return naw.TfIdfAug(model_path=f'./data/unlabeled/{dataset}tfidf/', tokenizer=_tokenizer, aug_p=aug_p)\n    elif arg == 'capital':\n        return CapitalizeAug(aug_p=aug_p)\n    elif arg == 'abbr':\n        return AbbrAug(aug_p=aug_p)\n    elif arg == 'repeatchar':\n        return RepeatCharAug(aug_word_p=aug_p)\n    elif arg == 'none':\n        return None\n    else:\n        raise ValueError(\"Unsupported augmentor!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:52.364166Z","iopub.execute_input":"2024-05-06T03:39:52.364538Z","iopub.status.idle":"2024-05-06T03:39:52.374099Z","shell.execute_reply.started":"2024-05-06T03:39:52.364509Z","shell.execute_reply":"2024-05-06T03:39:52.373086Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#for unsupervised data aug \nif opt.unsup:\n        #'real-time' means augmenting the texts on-the-fly\n        if opt.aug_type == 'real-time':\n            aug = opt.aug_choice\n            unlabeled_dataset = T5AugDataset(\n                src_file=unsup_file,\n                augmentor=aug,\n                max_len=opt.max_len,\n                tokenizer=tokenizer,\n                aug_p=opt.aug_p,\n                dataset=opt.dataset\n            )\n        else:\n            # Otherwise, augment all the texts beforehand\n            unlabeled_dataset = T5UnsupDataset(\n                src_file=f\"data/unlabeled/{opt.dataset.upper()}_200k_inf.txt\",\n                aug_file=f\"data/unlabeled/{opt.dataset.upper()}_200k_inf_{opt.aug_choice}.txt\",\n                max_len=opt.max_len,\n                tokenizer=tokenizer)\n\n        unsup_loader = DataLoader(unlabeled_dataset,\n                              num_workers=10,\n                              batch_size=opt.unsup_batch_size,\n                              shuffle=True)\n        print('[Info] {} instances from unlabeled set'.format(len(unlabeled_dataset)))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:39:56.375246Z","iopub.execute_input":"2024-05-06T03:39:56.375658Z","iopub.status.idle":"2024-05-06T03:39:57.165917Z","shell.execute_reply.started":"2024-05-06T03:39:56.375626Z","shell.execute_reply":"2024-05-06T03:39:57.164919Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"[Info] 200000 instances from unlabeled set\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"loss_fn = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\noptimizer = ScheduledOptim(torch.optim.Adam(filter(lambda x: x.requires_grad, model.parameters()),betas=(0.9, 0.98), eps=1e-09), opt.lr, 10000)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T03:40:01.998450Z","iopub.execute_input":"2024-05-06T03:40:01.998869Z","iopub.status.idle":"2024-05-06T03:40:02.011960Z","shell.execute_reply.started":"2024-05-06T03:40:01.998838Z","shell.execute_reply":"2024-05-06T03:40:02.010984Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def main():\n    tab = 0\n    eval_loss = 1e8\n    total_loss_ce = []\n    total_loss_unsup = []\n    best_bleu = 0.0\n    best_acc = 0.0\n\n    perp_list_all = SkipList()\n    bleu_list_all = SkipList()\n\n    #count filtered samples\n    num_all = 0\n    num_chosen = 0\n\n    start = time.time()\n    train_iter = iter(train_loader)\n    if opt.unsup:\n        unsup_iter = iter(unsup_loader)\n    for step in range(1, opt.steps):\n        try:\n            data = next(train_iter)\n        except:\n            train_iter = iter(train_loader)\n            data = next(train_iter)\n\n        if opt.unsup and step > opt.pre_step:\n            try:\n                unsup_batch = next(unsup_iter)\n            except:\n                unsup_iter = iter(unsup_loader)\n                unsup_batch = next(unsup_iter)\n\n        # supervised loss\n        lm_labels = data['target_ids'].to(device, dtype=torch.long)\n        lm_labels[lm_labels[:, :] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype=torch.long)\n        mask = data['source_mask'].to(device, dtype=torch.long)\n\n        outputs = model(input_ids=ids, attention_mask=mask, labels=lm_labels)\n        loss_ce = outputs[0]\n        total_loss_ce.append(loss_ce.item())\n\n        # unsupervised loss\n        if opt.unsup and step > opt.pre_step:\n            unsup_ids = unsup_batch['source_ids'].to(device, dtype=torch.long)\n            unsup_mask = unsup_batch['source_mask'].to(device, dtype=torch.long)\n            aug_ids = unsup_batch['augment_ids'].to(device, dtype=torch.long)\n            aug_mask = unsup_batch['augment_mask'].to(device, dtype=torch.long)\n            model.eval()\n            pseudo_labels = model.generate(unsup_ids,\n                                           attention_mask=unsup_mask,\n                                           num_beams=5,\n                                           max_length=30)\n            model.train()\n\n            if opt.filter == 'lm':\n                pseudo_targets = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for\n                                  g in\n                                  pseudo_labels]\n                perplexities = score_sentence(pseudo_targets, lm_model)\n                if num_all < len(unlabeled_dataset):\n                    for score in perplexities:\n                        perp_list_all.insert(score, None)\n                    if step > opt.n_step + opt.pre_step:\n                        idx = math.floor((1 - opt.sigma) * len(perp_list_all))\n                        perp_threshold = perp_list_all[idx][0]\n                    else:\n                        perp_threshold = 10000\n                else:\n                    perp_threshold = perp_threshold\n\n\n                y_mask = (torch.Tensor(perplexities) < perp_threshold).to(device, dtype=torch.float)\n                num_all += opt.unsup_batch_size\n                num_chosen += sum(y_mask)\n                fitlog.add_metric(num_chosen, name=\"num_chosen\", step=step)\n                fitlog.add_metric(num_all, name=\"num_all\", step=step)\n                fitlog.add_metric(num_chosen / num_all, name=\"filter_ratio\", step=step)\n                fitlog.add_metric(perp_threshold, name=\"perp_threshold\", step=step)\n\n                if num_all >= len(unlabeled_dataset):\n                    num_all = 0\n                    num_chosen = 0\n                    perp_list_all = SkipList()\n\n                pseudo_labels[pseudo_labels[:, :] == tokenizer.pad_token_id] = -100\n                unsup_output = model(aug_ids, attention_mask=aug_mask,\n                                     labels=pseudo_labels)\n\n                unsup_logits = unsup_output[1]\n                pseudo_labels[pseudo_labels[:, :] == -100] = tokenizer.pad_token_id\n                unsup_loss_ce = loss_fn(unsup_logits.view(-1, unsup_logits.size(-1)), pseudo_labels.view(-1))\n                y_mask = y_mask.unsqueeze(1).repeat(1, pseudo_labels.size(-1)).view(-1)\n\n                unsup_loss_ce = unsup_loss_ce * y_mask\n                unsup_loss_ce = unsup_loss_ce.sum()\n                if unsup_loss_ce > 0:\n                    unsup_loss_ce /= y_mask.sum()\n                total_loss_unsup.append(unsup_loss_ce.item())\n\n            if opt.filter == 'bleu':\n                pseudo_targets = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for\n                                  g in\n                                  pseudo_labels]\n                sources = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for\n                                  g in\n                                  unsup_ids]\n                sent_bleus = sentence_bleu_score(pseudo_targets, sources, opt.ngrams)\n\n                if num_all < len(unlabeled_dataset):\n                    for score in sent_bleus:\n                        bleu_list_all.insert(score, None)\n                    if step > opt.n_step + opt.pre_step:\n\n                        idx = math.floor(opt.sigma * len(bleu_list_all))\n                        bleu_threshold = bleu_list_all[idx][0]\n\n                    else:\n                        bleu_threshold = -1\n                else:\n                    #meaningless, just for notes\n                    bleu_threshold = bleu_threshold\n\n                y_mask = (torch.Tensor(sent_bleus) > bleu_threshold).to(device, dtype=torch.float)\n                num_all += opt.unsup_batch_size\n                num_chosen += sum(y_mask)\n                fitlog.add_metric(num_chosen, name=\"num_chosen\", step=step)\n                fitlog.add_metric(num_all, name=\"num_all\", step=step)\n                fitlog.add_metric(num_chosen / num_all, name=\"filter_ratio\", step=step)\n                fitlog.add_metric(bleu_threshold, name=\"bleu_threshold\",step=step)\n\n                if num_all >= len(unlabeled_dataset):\n                    num_all = 0\n                    num_chosen = 0\n                    bleu_list_all = SkipList()\n\n                pseudo_labels[pseudo_labels[:, :] == tokenizer.pad_token_id] = -100\n                unsup_output = model(aug_ids, attention_mask=aug_mask,\n                                     labels=pseudo_labels)\n                # unsup_loss_ce = unsup_output[0]\n                unsup_logits = unsup_output[1]\n                pseudo_labels[pseudo_labels[:, :] == -100] = tokenizer.pad_token_id\n                unsup_loss_ce = loss_fn(unsup_logits.view(-1, unsup_logits.size(-1)), pseudo_labels.view(-1))\n                y_mask = y_mask.unsqueeze(1).repeat(1, pseudo_labels.size(-1)).view(-1)\n\n                unsup_loss_ce = unsup_loss_ce * y_mask\n                unsup_loss_ce = unsup_loss_ce.sum()\n                if unsup_loss_ce > 0:\n                    unsup_loss_ce /= y_mask.sum()\n                total_loss_unsup.append(unsup_loss_ce.item())\n\n            if opt.filter == \"none\":\n                pseudo_labels[pseudo_labels[:, :] == tokenizer.pad_token_id] = -100\n                unsup_output = model(aug_ids, attention_mask=aug_mask,\n                                     labels=pseudo_labels)\n                unsup_loss_ce = unsup_output[0]\n                total_loss_unsup.append(unsup_loss_ce.item())\n\n            if opt.filter == \"cls\":\n                pseudo_targets = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for\n                                  g in\n                                  pseudo_labels]\n                pseudo_tgt = [cls_tokenizer.encode(line.strip())[:opt.max_len] for line in pseudo_targets]\n                pseudo_tgt = prepare_batch_cls(pseudo_tgt, pad_token_id=cls_tokenizer.pad_token_id).to(device)\n                logits_tgt = F.softmax(cls(pseudo_tgt), dim=-1)\n                pseudo_source = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g\n                                 in\n                                 unsup_ids]\n                pseudo_src = [cls_tokenizer.encode(line.strip())[:opt.max_len] for line in pseudo_source]\n                pseudo_src = prepare_batch_cls(pseudo_src, pad_token_id=cls_tokenizer.pad_token_id).to(device)\n                logits_src = F.softmax(cls(pseudo_src), dim=-1)\n                y_mask = (logits_tgt[:, 1-opt.style] - logits_src[:, 1-opt.style] > opt.phi).float()\n\n                num_all += opt.unsup_batch_size\n                num_chosen += sum(y_mask)\n                fitlog.add_metric(num_chosen, name=\"num_chosen\", step=step)\n                fitlog.add_metric(num_all, name=\"num_all\", step=step)\n                fitlog.add_metric(num_chosen / num_all, name=\"filter_ratio\", step=step)\n\n                if num_all >= len(unlabeled_dataset):\n                    num_all = 0\n                    num_chosen = 0\n\n                pseudo_labels[pseudo_labels[:, :] == tokenizer.pad_token_id] = -100\n                unsup_output = model(aug_ids, attention_mask=aug_mask,\n                                     labels=pseudo_labels)\n                unsup_logits = unsup_output[1]\n                pseudo_labels[pseudo_labels[:, :] == -100] = tokenizer.pad_token_id\n                unsup_loss_ce = loss_fn(unsup_logits.view(-1, unsup_logits.size(-1)), pseudo_labels.view(-1))\n                y_mask = y_mask.unsqueeze(1).repeat(1, pseudo_labels.size(-1)).view(-1)\n\n                unsup_loss_ce = unsup_loss_ce * y_mask\n                unsup_loss_ce = unsup_loss_ce.sum()\n                if unsup_loss_ce > 0:\n                    unsup_loss_ce /= y_mask.sum()\n\n                total_loss_unsup.append(unsup_loss_ce.item())\n\n        if opt.unsup and step > opt.pre_step:\n            optimize(optimizer, loss_ce + opt.weight * unsup_loss_ce)\n        else:\n            optimize(optimizer, loss_ce)\n\n        if step % opt.log_step == 0:\n            lr = optimizer._optimizer.param_groups[0]['lr']\n            if opt.unsup and step > opt.pre_step:\n                print('[Info] steps {:05d} | loss_sup {:.4f} | '\n                      'loss_unsup {:.4f} | lr {:.6f} | second {:.2f}'.format(\n                    step, np.mean(total_loss_ce), np.mean(total_loss_unsup)\n                    , lr, time.time() - start))\n                fitlog.add_loss(np.mean(total_loss_ce), name=\"Sup-loss\", step=step)\n                fitlog.add_loss(np.mean(total_loss_unsup), name=\"Unsup-loss\", step=step)\n            else:\n                print('[Info] steps {:05d} | loss_ce {:.4f} | lr {:.6f} | second {:.2f}'.format(\n                    step, np.mean(total_loss_ce), lr, time.time() - start))\n                fitlog.add_loss(np.mean(total_loss_ce), name=\"Sup-loss\", step=step)\n\n            total_loss_ce = []\n            total_loss_unsup = []\n            start = time.time()\n\n        if ((len(train_loader) > opt.eval_step\n             and step % opt.eval_step == 0)\n                or (len(train_loader) < opt.eval_step\n                    and step % len(train_loader) == 0)):\n\n            print(\"validation starts...\")\n            # if eval_loss >= valid_loss:\n            model.eval()\n\n            start = time.time()\n            pred_list = []\n\n            if not os.path.exists(f'./data/{opt.dataset}/outputs/{opt.model}/'):\n                os.mkdir(f'./data/{opt.dataset}/outputs/{opt.model}/')\n            with open('./data/{}/outputs/{}/{}_{}_{}.{}_step{}.txt'.format(opt.dataset, opt.model,\n                    opt.model, opt.dataset, opt.order, opt.style, step), 'w') as fout:\n                for idx, data in enumerate(val_loader):\n                    if idx % 10 == 0:\n                        print('[Info] processing {} batches | seconds {:.4f}'.format(\n                            idx, time.time() - start))\n                        start = time.time()\n\n                    ids = data['source_ids'].to(device, dtype=torch.long)\n                    mask = data['source_mask'].to(device, dtype=torch.long)\n                    generated_ids = model.generate(ids,\n                                                   attention_mask=mask,\n                                                   num_beams=5,\n                                                   max_length=30)\n                    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in\n                             generated_ids]\n                    pred_list.extend(preds)\n\n                for text in pred_list:\n                    fout.write(text.strip() + '\\n')\n\n            model.train()\n\n            pred_file = './data/{}/outputs/{}/{}_{}_{}.{}_step{}.txt'.format(opt.dataset, opt.model,\n                    opt.model, opt.dataset, opt.order, opt.style, step)\n\n            bleu = evaluate_bleu(val_label_files, pred_file)\n            print(bleu)\n            fitlog.add_metric(bleu, name=\"BLEU\", step=step)\n            _, perplexity = score_generated_sentences(pred_file, lm_model)\n            fitlog.add_metric(perplexity, name=\"perplexity\", step=step)\n\n            if bleu > best_bleu:\n                tab = 0\n                best_bleu = bleu\n                torch.save(model.state_dict(), 'checkpoints/{}_{}_{}_{}.chkpt'.format(\n                    opt.model, opt.dataset, opt.order, opt.style))\n                print('[Info] The checkpoint file has been updated.')\n                fitlog.add_best_metric({\"dev\":{\"BLEU\":best_bleu}})\n                test_bleu, test_acc, test_loss = test(model, tokenizer, cls, cls_tokenizer, opt)\n                test_hm = 2.0 / (1.0 / test_bleu + 100.0 / test_acc)\n                fitlog.add_loss({\"test\":{\"Loss\":test_loss}}, step=step)\n\n                test_file = './data/{}/outputs/{}/{}_{}_{}.{}_best_test.txt'.format(opt.dataset, opt.model,\n                                                                        opt.model, opt.dataset, opt.order, opt.style)\n                _, test_perplexity = score_generated_sentences(test_file, lm_model)\n                fitlog.add_best_metric({\"test\": {\"BLEU\": test_bleu, \"Acc\": test_acc, \"HM\": test_hm, \"perplexity\": test_perplexity}})\n            else:\n                tab += 1\n            if tab == opt.patience:\n                #early stopping\n                exit()\n\n            # Evaluate style accuracy\n            test_tgt = []\n            test_src = []\n            with open(pred_file, 'r') as f:\n                for line in f.readlines():\n                    if opt.style == 0:\n                        test_tgt.append(cls_tokenizer.encode(line.strip())[:opt.max_len])\n                    else:\n                        test_src.append(cls_tokenizer.encode(line.strip())[:opt.max_len])\n            cls_loader = SCIterator(test_src, test_tgt, opt, cls_tokenizer.pad_token_id)\n            cls_loss_fn = torch.nn.CrossEntropyLoss()\n\n            total_num = 0.\n            total_acc = 0.\n            total_loss = 0.\n            with torch.no_grad():\n                for i, batch in enumerate(cls_loader):\n                    x_batch, y_batch = map(lambda x: x.to(device), batch)\n                    logits = cls(x_batch)\n                    # print(F.softmax(logits, dim=-1))\n                    total_loss += cls_loss_fn(logits, y_batch)\n                    _, y_hat = torch.max(logits, dim=-1)\n                    same = [float(p == q) for p, q in zip(y_batch, y_hat)]\n                    total_acc += sum(same)\n                    total_num += len(y_batch)\n\n            print('Test: {}'.format('acc {:.4f}% | loss {:.4f}').format(\n                total_acc / total_num * 100, total_loss / total_num))\n\n            with open('./data/{}/outputs/{}/{}_{}_{}.{}_bleu.txt'.format(opt.dataset, opt.model,\n                    opt.model, opt.dataset, opt.order, opt.style), 'a') as fbl:\n\n                fbl.write('Bleu score at step {}: {:.4f};  Acc: {:.4f}\\n'.format(step, bleu, total_acc / total_num * 100))\n            acc = total_acc / total_num * 100\n            fitlog.add_metric(acc, name=\"Acc\", step=step)\n            if bleu == best_bleu:\n                best_acc = acc\n                fitlog.add_best_metric({\"dev\": {\"Acc\": best_acc}})\n\nfitlog.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T05:42:35.162492Z","iopub.execute_input":"2024-05-06T05:42:35.162801Z","iopub.status.idle":"2024-05-06T05:42:35.326226Z","shell.execute_reply.started":"2024-05-06T05:42:35.162777Z","shell.execute_reply":"2024-05-06T05:42:35.325023Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 329\u001b[0m\n\u001b[1;32m    326\u001b[0m                 best_acc \u001b[38;5;241m=\u001b[39m acc\n\u001b[1;32m    327\u001b[0m                 fitlog\u001b[38;5;241m.\u001b[39madd_best_metric({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_acc}})\n\u001b[0;32m--> 329\u001b[0m \u001b[43mfitlog\u001b[49m\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    332\u001b[0m     main()\n","\u001b[0;31mNameError\u001b[0m: name 'fitlog' is not defined"],"ename":"NameError","evalue":"name 'fitlog' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}